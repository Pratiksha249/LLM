# -*- coding: utf-8 -*-
"""Pratiksha_2448538LLM1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hpDwqU5PExnWNczgUsFyrH7D0rEhiyht
"""

import torch
from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering
import pandas as pd
import warnings
warnings.filterwarnings("ignore")

# Install required libraries
!pip install transformers==4.44.2 torch==2.4.1 pandas==2.2.2

# Initialize tokenizer and model (pre-trained on SQuAD)
print("=== Initializing Model ===")
print("Using pre-trained 'distilbert-base-uncased-distilled-squad' to avoid dataset loading issues.")
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')
model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')

# Custom question-answering function (no pipeline)
def answer_question(question, context):
    # Tokenize inputs
    inputs = tokenizer(
        question,
        context,
        add_special_tokens=True,
        max_length=512,
        truncation=True,
        return_tensors='pt'
    )
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    # Run inference
    model.to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)

    # Extract answer span
    start_idx = torch.argmax(outputs.start_logits, dim=1).item()
    end_idx = torch.argmax(outputs.end_logits, dim=1).item() + 1
    answer_tokens = input_ids[0][start_idx:end_idx]
    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)

    # Handle edge cases
    if not answer.strip():
        answer = "Unable to extract a valid answer."
    return answer

# Context about Christ University
context = """
Christ University, originally established as Christ College in 1969, is a prominent private deemed-to-be-university located in Bengaluru, Karnataka, India. It was officially declared a "Deemed to be University" under Section 3 of the UGC Act, 1956, by the Ministry of Human Resource Development, Government of India, on July 22, 2008. It is administered by the Carmelites of Mary Immaculate (CMI), a Catholic religious congregation. Christ University has a strong reputation, being one of the first institutions in India to be accredited by the National Assessment and Accreditation Council (NAAC) in 1998. It currently holds an 'A+' Grade from NAAC. In NIRF 2024, it was ranked 60th in the university category. The university has multiple campuses: Central Campus, Bannerghatta Road Campus, Kengeri Campus, and Yeshwanthpur Campus (opened in August 2022) in Bengaluru, as well as campuses in Lavasa (Pune, Maharashtra) and Ghaziabad (Delhi NCR). It offers a wide range of undergraduate, postgraduate, and doctoral programs across Humanities, Social Sciences, Sciences, Commerce, Management, Engineering, Architecture, Education, and Law. The university has over 30,000 students, including a significant international student community. Christ University emphasizes holistic education, academic discipline, and innovative curricula, with initiatives like the Centre for Social Action (CSA) for social awareness and community development. Admissions are typically through the Christ University Entrance Test (CUET), followed by a Micro Presentation (MP) and Personal Interview (PI).
"""

# Part A: Descriptive Question Answering System
print("\n=== Part A: Descriptive Question Answering System ===")
print("Custom implementation using fine-tuned DistilBERT with Christ University context.")
print("Learning Focus: Manual tokenization, inference, and answer extraction.")

# Part B: Prompt Engineering and Optimization
print("\n=== Part B: Prompt Engineering and Optimization ===")
print("Testing three prompt variations to analyze their impact.")

# Define three identical prompts for testing across models
question_base = "When was Christ University established?"
prompt_variations = [
    question_base,  # Direct prompt
    f"Based on the information about Christ University, {question_base}",  # Contextual prompt
    f"As an education expert, provide a precise answer to: {question_base}"  # Role-based prompt
]

# Store results for DistilBERT
results = []
for i, prompt in enumerate(prompt_variations, 1):
    answer = answer_question(prompt, context)
    results.append({"Prompt": prompt, "Answer": answer})
    print(f"\nPrompt {i}: {prompt}")
    print(f"Answer: {answer}")

# Insights on Prompt Engineering
print("\n=== Insights on Prompt Engineering ===")
print("1. **Direct Prompt**: Simple and effective for extractive models like DistilBERT, but may lack focus for complex questions.")
print("2. **Contextual Prompt**: Specifying the context improves relevance, as it directs the model to the provided text.")
print("3. **Role-based Prompt**: Assigning a role (e.g., 'education expert') has minimal impact on DistilBERT but can enhance tone in generative models like Gemini or ChatGPT.")
print("Optimization Tip: For extractive models, use concise, context-specific prompts to avoid token truncation and improve accuracy. For generative models, role-based prompts can refine response style.")

# Part C: Comparison Across Models
print("\n=== Part C: Comparison Across Models ===")
print("Testing three identical prompts across DistilBERT, Gemini, and ChatGPT.")
test_prompts = prompt_variations
print(f"Test Prompts: {test_prompts}")

# DistilBERT responses
distilbert_answers = [answer_question(prompt, context) for prompt in test_prompts]
for i, (prompt, answer) in enumerate(zip(test_prompts, distilbert_answers), 1):
    print(f"\nDistilBERT Prompt {i}: {prompt}")
    print(f"DistilBERT Answer: {answer}")

# Instructions for Gemini and ChatGPT
print("\nInstructions for Gemini and ChatGPT:")
print("1. Gemini: Access Gemini via its official interface (e.g., Google Cloud or API). Input each of the three test prompts and record the responses.")
print("2. ChatGPT: Access ChatGPT via OpenAI's platform or API. Input each of the three test prompts and record the responses.")
print("3. Create a DataFrame to compare responses:")
print("   - Column 1: Model (DistilBERT, Gemini, ChatGPT)")
print("   - Column 2: Prompt")
print("   - Column 3: Response")
print("   - Column 4: Notes (e.g., accuracy, fluency, conciseness)")

# Placeholder for comparison
comparison_data = []
for i, prompt in enumerate(test_prompts):
    comparison_data.append({"Model": "DistilBERT", "Prompt": prompt, "Response": distilbert_answers[i], "Notes": "Extractive, concise, pre-trained on SQuAD."})
    comparison_data.append({"Model": "Gemini", "Prompt": prompt, "Response": "[Run prompt in Gemini]", "Notes": "[Analyze tone, detail, accuracy]"})
    comparison_data.append({"Model": "ChatGPT", "Prompt": prompt, "Response": "[Run prompt in ChatGPT]", "Notes": "[Analyze tone, detail, accuracy]"})

comparison_df = pd.DataFrame(comparison_data)
print("\nComparison Table:")
print(comparison_df)

print("\n=== Insights on Model Differences ===")
print("- **DistilBERT**: Extractive, pulls exact text (e.g., '1969'), accurate but limited fluency.")
print("- **Gemini**: Expected to rephrase responses (e.g., 'Christ University was established in 1969 as Christ College'), with concise and context-aware answers.")
print("- **ChatGPT**: Likely verbose, adding details (e.g., 'Christ University, originally Christ College, was established in 1969 in Bengaluru').")
print("Analysis Focus: Compare accuracy (correct year: 1969), fluency (natural language quality), and conciseness (response length).")

# Part D: Sample Questions and Answers
print("\n=== Part D: Sample Questions and Answers ===")
sample_questions = [
    "What is the accreditation status of Christ University?",
    "How many students are enrolled at Christ University?",
    "What types of programs does Christ University offer?"
]

print("Generating answers for three sample questions:")
sample_results = []
for i, question in enumerate(sample_questions, 1):
    answer = answer_question(question, context)
    sample_results.append({"Question": question, "Answer": answer})
    print(f"\nQuestion {i}: {question}")
    print(f"Answer: {answer}")

# Display sample results
sample_df = pd.DataFrame(sample_results)
print("\nSample Questions and Answers Table:")
print(sample_df)

